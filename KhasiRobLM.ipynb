{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"o8gbn-PcqvVp"},"outputs":[],"source":["!pip install transformers"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"9QR-lGrF-mM7","executionInfo":{"status":"ok","timestamp":1691042145621,"user_tz":-330,"elapsed":6697,"user":{"displayName":"shanborwill warjri","userId":"13494770003655537016"}}},"outputs":[],"source":["from tqdm.auto import tqdm\n","import torch\n","import shutil\n","from tokenizers import BertWordPieceTokenizer\n","import time\n","from transformers import BertTokenizer"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"p7jlW9v2udT_","executionInfo":{"status":"ok","timestamp":1691042147333,"user_tz":-330,"elapsed":6,"user":{"displayName":"shanborwill warjri","userId":"13494770003655537016"}}},"outputs":[],"source":["model_path='./RoPOS_Wordpiecetok1'"]},{"cell_type":"markdown","metadata":{"id":"hvu9BULIrJjN"},"source":[]},{"cell_type":"code","execution_count":6,"metadata":{"id":"2ObGjdzpoE_D","executionInfo":{"status":"ok","timestamp":1691042148184,"user_tz":-330,"elapsed":855,"user":{"displayName":"shanborwill warjri","userId":"13494770003655537016"}}},"outputs":[],"source":["tokenizer = BertTokenizer.from_pretrained(model_path)"]},{"cell_type":"markdown","metadata":{"id":"XIUIxujZo53E"},"source":["Loading the files"]},{"cell_type":"markdown","metadata":{"id":"VVXXn1hGqAz_"},"source":["Create the input pipeline"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"r0cITBZcqQes","executionInfo":{"status":"ok","timestamp":1691042151885,"user_tz":-330,"elapsed":4,"user":{"displayName":"shanborwill warjri","userId":"13494770003655537016"}}},"outputs":[],"source":["def mlm(tensor):\n","  rand = torch.rand(tensor.shape)\n","  mask_arr = (rand < .15) * (tensor>2)\n","  for i in range(tensor.shape[0]):\n","    # get indices of mask positions from mask array\n","    selection = torch.flatten(mask_arr[i].nonzero()).tolist()\n","    # mask input_ids\n","    tensor[i, selection] = 4  # our custom [MASK] token == 3\n","  return tensor\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ySoDh1SUrdx1"},"outputs":[],"source":["input_ids=[]\n","attention_mask=[]\n","labels=[]\n","input_ids_temp=[]\n","#for path in tqdm(paths):\n","with open('./data/articles.txt','r',encoding='utf-8')as f:\n","    lines = f.read().split('\\n')\n","sample=tokenizer(lines, max_length=512, padding='max_length', truncation=True,return_tensors='pt')\n","labels.append(sample.input_ids)\n","attention_mask.append(sample.attention_mask)\n","input_ids_temp.append(sample.input_ids)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l_ScD9nK4-JU"},"outputs":[],"source":["input_ids=[]\n","input_ids.append(mlm(sample.input_ids.detach().clone()))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zQvft4nbt9Oe"},"outputs":[],"source":["input_ids=torch.cat(input_ids)\n","attention_mask=torch.cat(attention_mask)\n","labels=torch.cat(labels)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ml5gY8sguq4g"},"outputs":[],"source":["encodings = {'input_ids': input_ids,\n","             'attention_mask': attention_mask,\n","             'labels': labels}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eZfFuAE5YdhR"},"outputs":[],"source":["class Dataset(torch.utils.data.Dataset):\n","    def __init__(self, encodings):\n","        # store encodings internally\n","        self.encodings = encodings\n","\n","    def __len__(self):\n","        # return the number of samples\n","        return self.encodings['input_ids'].shape[0]\n","\n","    def __getitem__(self, i):\n","        # return dictionary of input_ids, attention_mask, and labels for index i\n","        return {key: tensor[i] for key, tensor in self.encodings.items()}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t_ANsAM2vSHi"},"outputs":[],"source":["dataset = Dataset(encodings)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z45Ht74eYjgO"},"outputs":[],"source":["dataloader = torch.utils.data.DataLoader(dataset, batch_size=8, shuffle=True)"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"QkqVDsYaYoCf","executionInfo":{"status":"ok","timestamp":1691042172178,"user_tz":-330,"elapsed":5,"user":{"displayName":"shanborwill warjri","userId":"13494770003655537016"}}},"outputs":[],"source":["from transformers import RobertaConfig\n","\n","config = RobertaConfig(\n","    vocab_size=30_522,  # we align this to the tokenizer vocab_size\n","    max_position_embeddings=514,\n","    hidden_size=768,\n","    num_attention_heads=12,\n","    hidden_states=True,\n","    num_hidden_layers=6,\n","    type_vocab_size=1\n",")"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"YIFhPVQ8YrMw","executionInfo":{"status":"ok","timestamp":1691042177159,"user_tz":-330,"elapsed":1784,"user":{"displayName":"shanborwill warjri","userId":"13494770003655537016"}}},"outputs":[],"source":["from transformers import RobertaForMaskedLM\n","model = RobertaForMaskedLM(config)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5wzr5ZhJYunP"},"outputs":[],"source":["device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","# and move our model over to the selected device\n","model.to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nH6fa9C7Yw7B"},"outputs":[],"source":["from transformers import AdamW\n","# initialize optimizer\n","optim = AdamW(model.parameters(), lr=1e-4)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zchrMvDOAJM1"},"outputs":[],"source":["def save_ckp(epoch, network, optimizer, loss, is_best, checkpoint_dir, best_model_dir):\n","    checkpoint = {\n","        'epoch': epoch,\n","        'model': network,\n","        'model_state_dict': network.state_dict(),\n","        'optimizer': optimizer.state_dict(),\n","        'loss': loss\n","    }\n","    f_path = checkpoint_dir + '/checkpointWPtok1.pt'\n","    torch.save(checkpoint, f_path)\n","    if is_best:\n","        best_fpath = best_model_dir + '/best_modelWPtok1.pt'\n","        shutil.copyfile(f_path, best_fpath)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6qpGFsyNygh6"},"outputs":[],"source":["def load_ckp(checkpoint_fpath):\n","    checkpoint = torch.load(checkpoint_fpath)\n","\n","    return checkpoint"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FzNQQbBSyt69"},"outputs":[],"source":["ft=False"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"vA3ydGXg06In"},"outputs":[],"source":["epochs = 10\n","start_epoch=0\n","save_chpt=20\n","start_time=time.time()\n","time_count=1\n","if ft==False:\n","  if start_epoch==0: best_loss=float('inf')\n","  ckp_load=load_ckp('./CheckPoint/checkpointWPtok1.pt')\n","  model.load_state_dict(ckp_load['model_state_dict'])\n","  model.to(device)\n","  optim= AdamW(model.parameters(),lr = 1e-4)\n","  optim.load_state_dict(ckp_load['optimizer'])\n","  for param_group in optim.param_groups:\n","     param_group['lr']=1e-5\n","  start_epoch=ckp_load['epoch']\n","  print(start_epoch)\n","  best_loss=ckp_load['loss']\n","\n","for epoch in range(start_epoch,epochs):\n","\n","  if start_epoch==0: best_loss=float('inf')\n","  is_best=False\n","    # setup loop with TQDM and dataloader\n","  loop = tqdm(dataloader, leave=True)\n","  for batch in loop:\n","    total_train_loss = 0\n","    optim.zero_grad()\n","    input_ids = batch['input_ids'].to(device)\n","    attention_mask = batch['attention_mask'].to(device)\n","    labels = batch['labels'].to(device)\n","    outputs = model(input_ids=input_ids, attention_mask=attention_mask,labels=labels)\n","    loss = outputs.loss\n","    total_train_loss += float(loss)\n","    loss.backward()\n","    optim.step()\n","    checkpoint_dir='./CheckPoint'\n","    model_dir='./BestModel'\n","    end_time=time.time()\n","    if save_chpt>0 and (((end_time-start_time)//60)>=20):#saving after every 20 mins because of google colab intermittent unmounting og drive\n","      print(f\"saving after {time_count*save_chpt} mins\")\n","      save_ckp(epoch+1,model,optim,best_loss,is_best, checkpoint_dir, model_dir)\n","      model.save_pretrained('./KhasiPretrained_mins')\n","      start_time=end_time\n","      time_count+=1\n","    loop.set_description(f'Epoch {epoch}')\n","    loop.set_postfix(loss=loss.item())\n","  avg_train_loss=total_train_loss/len(batch)\n","  if epoch==0:\n","    best_loss=avg_train_loss\n","  else:\n","    if best_loss>avg_train_loss:\n","      is_best=True\n","      best_loss=avg_train_loss\n","      model.save_pretrained('./KhasiPretrained')\n","  print(f'epoch : {epoch} Average training Loss: {avg_train_loss}')\n","  #model.save_pretrained('./RoPOS/JBallRoPOStok1')"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"eB0qwM6HY9KH","executionInfo":{"status":"ok","timestamp":1691042194846,"user_tz":-330,"elapsed":4795,"user":{"displayName":"shanborwill warjri","userId":"13494770003655537016"}}},"outputs":[],"source":["from transformers import pipeline"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"UoAHulvnY_23","executionInfo":{"status":"ok","timestamp":1691042442660,"user_tz":-330,"elapsed":3922,"user":{"displayName":"shanborwill warjri","userId":"13494770003655537016"}}},"outputs":[],"source":["fill = pipeline('fill-mask', model='./KhasiPretrained', tokenizer=tokenizer)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YmKOlhXhZCKx"},"outputs":[],"source":["fill(f'hadien shi {fill.tokenizer.mask_token} jong ka   {fill.tokenizer.mask_token} vote ?')"]}],"metadata":{"colab":{"machine_shape":"hm","provenance":[{"file_id":"1TTf2el09E57Jd6YRe4IqUpNxC5lI9x-u","timestamp":1691038156680}],"gpuClass":"premium"},"gpuClass":"premium","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}