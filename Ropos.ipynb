{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"_GOObPJ9iJsq"},"outputs":[],"source":["from numpy.random import seed\n","seed(1)\n","import  keras\n","import numpy as np\n","import json\n","from keras.preprocessing.text import Tokenizer\n","from sklearn.metrics import recall_score, precision_score, classification_report, accuracy_score, confusion_matrix, f1_score\n","from tqdm import tqdm_notebook\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import time\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BIq01IfECzh8"},"outputs":[],"source":["!pip install transformers"]},{"cell_type":"code","source":["from tokenizers import BertWordPieceTokenizer\n","from transformers import BertTokenizer"],"metadata":{"id":"v4mmnf5liAf1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cd drive/MyDrive/for-github/KhasiRobertaPOS"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eg82KxgDHgAz","executionInfo":{"status":"ok","timestamp":1690994372764,"user_tz":-330,"elapsed":11,"user":{"displayName":"shanborwill warjri","userId":"13494770003655537016"}},"outputId":"cc70f691-4c9e-4dfd-c81d-a130aa314031"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/for-github/KhasiRobertaPOS\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k9FGrIa1iJsu"},"outputs":[],"source":["tokenizer = BertTokenizer.from_pretrained('RoPOS_Wordpiecetok1')\n","from sklearn.model_selection import train_test_split"]},{"cell_type":"code","source":["!python src/metrics.py\n","from src.metrics import categorical_accuracy,epoch_time,format_time"],"metadata":{"id":"2mqCdLIYMc03"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oulKeINKiJs3"},"outputs":[],"source":["\n","!python ./src/batch.py\n","from src.batch import make_batches,  make_test_batches\n","\n","#!python src/training.py\n","from src.training import train,evaluate, test\n","\n","\n","!python src/utils.py\n","from src.utils import *"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FbsF1f_SiJst"},"outputs":[],"source":["Y=[]\n","X=[]\n","with open('data/tags.txt', 'r',encoding='utf-8') as fp:\n","    for line in fp:\n","      wordlst=[]\n","      for word in line.split():\n","         wordlst.append(word.lower())\n","      if len(wordlst)>0:\n","        Y.append(wordlst)\n","with open('data/text.txt', 'r',encoding='utf-8') as fp:\n","    for line in fp:\n","      wordlst=[]\n","      for word in line.split():\n","         wordlst.append(word.lower())\n","      if len(wordlst)>0:   #print(wordlst)\n","        X.append(wordlst)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P5uwsZYFiJsu"},"outputs":[],"source":["TEST_SIZE = 0.1\n","train_text, test_text, train_label, test_label = train_test_split(X, Y, test_size=TEST_SIZE, random_state=4)\n","VALID_SIZE = 0.2\n","train_text, valid_text, train_label, valid_label = train_test_split(train_text, train_label, test_size=VALID_SIZE, random_state=4)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_KQWJDSIiJsv"},"outputs":[],"source":["with open('tagsconfig.json','r') as f:#load the saved json string tokenizer config\n","  con=json.load(f)\n","tag_tokenizer=keras.preprocessing.text.tokenizer_from_json(con)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"I25CITwKiJsv"},"outputs":[],"source":["class InputExample(object):\n","    def __init__(self, text_a, label=None):\n","        self.text_a = text_a\n","        self.label = label\n","\n","def create_tokenizer():\n","    tokenizer = BertTokenizer.from_pretrained('RoPOS_Wordpiecetok1')\n","    return tokenizer\n","\n","def convert_single_example(tokenizer, example, max_seq_length=512,max_id_length=200):\n","    tokens_a = example.text_a\n","    labels_a= example.label\n","    if len(tokens_a) > max_seq_length-2:\n","        tokens_a = tokens_a[0 : (max_seq_length-2)]\n","        labels_a =labels_a[0 : (max_seq_length-2)]\n","\n","    orig_to_tok_map = []\n","    tokens = []\n","    labels=[]\n","\n","    tokens.append(\"[CLS]\")\n","    labels.append(\"<pad>\")\n","\n","    orig_to_tok_map.append(len(tokens)-1)\n","    #print(len(tokens_a))\n","    for (i,token) in enumerate(tokens_a):\n","        orig_to_tok_map.append(len(tokens))#keep first piece\n","        sub_toks=tokenizer.tokenize(token)\n","        #print(sub_toks)\n","        tokens.extend(sub_toks)\n","        sub_labs=[labels_a[i]]*len(sub_toks)#repeat same labels for subtoks\n","        labels.extend(sub_labs)\n","\n","    tokens.append(\"[SEP]\")\n","    labels.append(\"<pad>\")\n","    # print('label:',len(labels),'\\t','toks:',len(tokens))\n","    orig_to_tok_map.append(len(tokens)-1)\n","    input_ids = tokenizer.convert_tokens_to_ids([tok for tok  in tokens])\n","    label_ids = []\n","    label_ids.extend([tag_tokenizer.word_index[label] for label in labels])\n","\n","    if len(input_ids)>max_id_length:\n","        input_ids=input_ids[0 : max_id_length]\n","        label_ids=label_ids[0 : max_id_length]\n","    input_mask = [1] * len(input_ids)\n","    # Zero-pad up to the sequence length.\n","    while len(input_ids) < max_id_length:\n","        input_ids.append(0)\n","        input_mask.append(0)\n","        label_ids.append(0)\n","\n","\n","    return input_ids, input_mask, orig_to_tok_map,label_ids\n","\n","def convert_examples_to_features(tokenizer, examples, max_seq_length=256):\n","    input_ids, input_masks, orig_to_tok_maps, labels = [], [], [], []\n","    for example in tqdm_notebook(examples, desc=\"Converting examples to features\"):\n","        input_id, input_mask,orig_to_tok_map, label = convert_single_example(\n","            tokenizer, example, max_seq_length\n","        )\n","        input_ids.append(input_id)\n","        input_masks.append(input_mask)\n","        labels.append(label)\n","\n","    return (\n","        np.array(input_ids),\n","        np.array(input_masks),\n","        np.array(labels),\n","    )\n","\n","def convert_text_to_examples(texts, labels):\n","    InputExamples = []\n","    for text, label in zip(texts, labels):\n","        InputExamples.append(\n","            InputExample( text_a=text, label=label)\n","        )\n","    return InputExamples"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fNHguK2OiJsv"},"outputs":[],"source":["max_id_length=250\n","MAX_SEQUENCE_LENGTH = 200\n","tokenizer = create_tokenizer()\n","n_tags=len(tag_tokenizer.word_index)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"16CX8WMTiJsw"},"outputs":[],"source":["InputExamples = []\n","for text, label in zip(test_text[0:1], test_label[0:1]):\n","    InputExamples.append(\n","        InputExample( text_a=text, label=label)\n",")"]},{"cell_type":"markdown","metadata":{"id":"-KAaP_1conJ9"},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mpQSNftGiJsx"},"outputs":[],"source":["#Convert data to InputExample format\n","train_examples = convert_text_to_examples(train_text, train_label)\n","valid_examples = convert_text_to_examples(valid_text, valid_label)\n","test_examples = convert_text_to_examples(test_text, test_label) #uncomment this for test\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NmtZFFZIiJsx"},"outputs":[],"source":["# Convert to features\n","(train_input_ids, train_input_masks,  train_labels_ids\n",") = convert_examples_to_features(tokenizer, train_examples, max_seq_length=MAX_SEQUENCE_LENGTH+2)\n","(valid_input_ids, valid_input_masks,  valid_labels_ids\n",") = convert_examples_to_features(tokenizer, valid_examples, max_seq_length=MAX_SEQUENCE_LENGTH+2)\n","# (test_input_ids, test_input_masks,  test_labels_ids\n","# ) = convert_examples_to_features(tokenizer, test_examples, max_seq_length=MAX_SEQUENCE_LENGTH+2)#uncomment for test"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P8FauOeqiJs3"},"outputs":[],"source":["class RoPOStagger(nn.Module):\n","    def __init__(self,Rob_lang,output_dim, dropout):\n","        super().__init__()\n","        self.Rob_lang = Rob_lang\n","        embedding_dim = 768\n","        self.fc = nn.Linear(embedding_dim, output_dim)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def forward(self, text):\n","        out=self.Rob_lang(text)\n","        # print(len(out))  # 3\n","        hidden_states = out[1]\n","        embedding_output = hidden_states[0]\n","        embedded = self.dropout(embedding_output)\n","        predictions = self.fc(self.dropout(embedded))\n","\n","        predictions = self.fc(embedded)\n","       # print(predictions.shape)\n","        return predictions\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u5a9GHp-iJs3"},"outputs":[],"source":["from transformers import RobertaForMaskedLM\n","Rob_lang = RobertaForMaskedLM.from_pretrained('KhasiPretrained')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uw0Aa62uiJs3"},"outputs":[],"source":["OutputDim = 66#len tags\n","Dropout = .5\n","model = RoPOStagger(Rob_lang,OutputDim,Dropout)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"si-INee4iJs4"},"outputs":[],"source":["device=torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","model = model.to(device)\n","learn_rate = 1e-4\n","optimizer = optim.Adam(model.parameters(), lr = learn_rate)\n","TAG_PAD_IDX = 0\n","criterion = nn.CrossEntropyLoss(ignore_index = TAG_PAD_IDX)\n","criterion = criterion.to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7xioC7xCiJs5"},"outputs":[],"source":["import time\n","epochs =50\n","\n","best_valid_acc=0\n","# (batch_inputs,batch_tags) = make_batches(train_input_ids,train_labels_ids,8)\n","(batch_val_inputs,batch_val_tags) = make_batches(valid_input_ids,valid_labels_ids,8)\n","\n","for epoch in range(epochs):\n","    (batch_inputs,batch_tags) = make_batches(train_input_ids,train_labels_ids,8)\n","    start_time = time.time()\n","\n","    train_loss, train_acc = train(batch_inputs,batch_tags,model, optimizer, criterion, TAG_PAD_IDX,device)\n","    valid_loss, valid_acc = evaluate(batch_val_inputs,batch_val_tags,model,  criterion, TAG_PAD_IDX,device)\n","\n","    end_time = time.time()\n","\n","    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n","\n","\n","    if valid_acc > best_valid_acc:\n","        best_valid_acc = valid_acc\n","        torch.save(model.state_dict(), 'bestmodel.pt')\n","\n","    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n","    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n","    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n","#torch.save(model.state_dict(),'model.pt')\n","\n"]},{"cell_type":"markdown","source":["**Testing**"],"metadata":{"id":"lyyOXd7P0DSn"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"SBbIeFteiJs4"},"outputs":[],"source":["model.load_state_dict(torch.load('model_RoPOStok1_val_4.pt'))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8Gw42S46iJs6"},"outputs":[],"source":["(batch_test_inputs,batch_test_tags) = make_test_batches(test_input_ids,test_labels_ids,315)\n","test_loss, test_acc ,ypred= test(model,batch_test_inputs,batch_test_tags, criterion, TAG_PAD_IDX,device)\n","print(test_loss ,'\\t', test_acc)\n"]},{"cell_type":"markdown","source":["**Inferencing**"],"metadata":{"id":"18VqMYt60ePL"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"xKFLadSciJs8"},"outputs":[],"source":["id2tag={id:t for t,id in tag_tokenizer.word_index.items()}\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TDTlh24WiJs7"},"outputs":[],"source":["fout=open('testv2pos.txt', 'a+',encoding='utf-8')\n","with open('data/69.txt', 'r',encoding='utf-8') as fp:\n","    for line in fp:\n","       if len(line)>0:\n","            retags=tag_sentence(tokenizer,line,model,id2tag,device)\n","            wordlst=[word.lower() for word in line.split()]\n","            #print(retags)\n","            tagged_sent=join(retags,wordlst)\n","            #print(tagged_sent)\n","            fout.write(tagged_sent + '\\n')\n","fp.close()\n","fout.close()"]}],"metadata":{"colab":{"provenance":[{"file_id":"1da-12GW1SGkqE7iqBISDaiREtcsw1p32","timestamp":1691045585708}],"machine_shape":"hm","gpuType":"V100"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}